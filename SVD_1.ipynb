{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Vulnerability Detection using Deep Learning (Experiment Replication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicolumn experiment (All CWEs and others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is from Russell et. al work (Automated Vulnerability Detection in Source Code Using Deep Representation Learning) https://arxiv.org/abs/1807.04320\n",
    "* Datasets downloaded from https://osf.io/d45bw/\n",
    "* Datasets distribution: Training (80%), Validation (10%), Testing (10%)\n",
    "* The dataset consists of the source code of 1.27 million functions mined from open source software, labeled by static analysis for potential vulnerabilities.\n",
    "* Each function's raw source code, starting from the function name, is stored as a variable-length UTF-8 string. Five binary 'vulnerability' labels are provided for each function, corresponding to the four most common CWEs in our data plus all others: \n",
    " * CWE-120 (3.7% of functions)\n",
    " * CWE-119 (1.9% of functions)\n",
    " * CWE-469 (0.95% of functions)\n",
    " * CWE-476 (0.21% of functions)\n",
    " * CWE-other (2.7% of functions)\n",
    "* Functions may have more than one detected CWE each.\n",
    "* Python 3.6 and Tensorflow 2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the HDF5 files for training/validation/testing datasets to python pickle for ease of future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3 datasets available\n",
    "\n",
    "data = h5py.File(\"D:/Downloads/Dataset/VDISC_train.hdf5\",'r')\n",
    "data = h5py.File(\"D:/Downloads/Dataset/VDISC_validate.hdf5\",'r')\n",
    "data = h5py.File(\"D:/Downloads/Dataset/VDISC_test.hdf5\",'r')\n",
    "\n",
    "#data = h5py.File(\"VDISC_validate.hdf5\",'r')\n",
    "#data = h5py.File(\"VDISC_test.hdf5\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all groups\n",
    "data.visit(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe from the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "mydf = pd.DataFrame(list(data['functionSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "mydf['CWE-119']=list(data['CWE-119']); mydf['CWE-120']=list(data['CWE-120']); mydf['CWE-469']=list(data['CWE-469']); mydf['CWE-476']=list(data['CWE-476']); mydf['CWE-other']=list(data['CWE-other']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.rename(columns={0:'functionSource'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "mydf.iloc[0:5,0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "mydf.to_pickle(\"VDISC_train.pickle\")\n",
    "mydf.to_pickle(\"VDISC_validate.pickle\")\n",
    "mydf.to_pickle(\"VDISC_test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_pickle(\"VDISC_train.pickle\")\n",
    "validate=pd.read_pickle(\"VDISC_validate.pickle\")\n",
    "test=pd.read_pickle(\"VDISC_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONTINUE LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import pickle\n",
    "\n",
    "print(\"Tensorlfow version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting static and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random seed\n",
    "#myrand=np.random.randint(1, 99999 + 1)\n",
    "myrand=71926\n",
    "np.random.seed(myrand)\n",
    "tf.random.set_seed(myrand)\n",
    "print(\"Random seed is:\",myrand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global value\n",
    "WORDS_SIZE=10000\n",
    "INPUT_SIZE=500\n",
    "NUM_CLASSES=2\n",
    "MODEL_NUM=0\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_pickle(\"VDISC_train.pickle\")\n",
    "validate=pd.read_pickle(\"VDISC_validate.pickle\")\n",
    "test=pd.read_pickle(\"VDISC_test.pickle\")\n",
    "\n",
    "for dataset in [train,validate,test]:\n",
    "    for col in range(1,6):\n",
    "        dataset.iloc[:,col] = dataset.iloc[:,col].map({False: 0, True: 1})\n",
    "\n",
    "# Create source code sdata for tokenization\n",
    "x_all = train['functionSource']\n",
    "x_all = x_all.append(validate['functionSource'])\n",
    "x_all = x_all.append(test['functionSource'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the datasets\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the source codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer with word-level\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "\n",
    "# ? my code from : https://github.com/pavansolapure/opencodez-samples/issues/7 \n",
    "# tokenizer.fit_on_texts([x.decode('utf-8') for x in x_all])\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "tokenizer.fit_on_texts(list([x.decode('utf-8') for x in x_all]))\n",
    "del(x_all)\n",
    "print('Number of tokens: ',len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing to top N words\n",
    "tokenizer.num_words = WORDS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words\n",
    "sorted(tokenizer.word_counts.items(), key=lambda x:x[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequence files from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokkenizing train data and create matrix\n",
    "# x_train = tokenizer.texts_to_matrix(list([x.decode('utf-8') for x in train['functionSource']]), mode='binary')\n",
    "# x_validate = tokenizer.texts_to_matrix(list([x.decode('utf-8') for x in validate['functionSource']]), mode='binary')\n",
    "# x_test = tokenizer.texts_to_matrix(list([x.decode('utf-8') for x in test['functionSource']]), mode='binary')\n",
    "\n",
    "\n",
    "# list_tokenized_train = tokenizer.texts_to_sequences(train['functionSource'])\n",
    "# ? changed code\n",
    "list_tokenized_train = tokenizer.texts_to_sequences([x.decode('utf-8') for x in train['functionSource']])\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_train, \n",
    "                                  maxlen=INPUT_SIZE,\n",
    "                                  padding='post')\n",
    "x_train = x_train.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokkenizing test data and create matrix\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(test['functionSource'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences([x.decode('utf-8') for x in test['functionSource']])\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_test, \n",
    "                                 maxlen=INPUT_SIZE,\n",
    "                                 padding='post')\n",
    "x_test = x_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokkenizing validate data and create matrix\n",
    "# list_tokenized_validate = tokenizer.texts_to_sequences(validate['functionSource'])\n",
    "list_tokenized_validate = tokenizer.texts_to_sequences([x.decode('utf-8') for x in validate['functionSource']])\n",
    "x_validate = tf.keras.preprocessing.sequence.pad_sequences(list_tokenized_validate, \n",
    "                                 maxlen=INPUT_SIZE,\n",
    "                                 padding='post')\n",
    "x_validate = x_validate.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "test.iloc[0:5,1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Enconding (OHE) on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "y_test=[]\n",
    "y_validate=[]\n",
    "\n",
    "for col in range(1,6):\n",
    "    y_train.append(tf.keras.utils.to_categorical(train.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))\n",
    "    y_test.append(tf.keras.utils.to_categorical(test.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))\n",
    "    y_validate.append(tf.keras.utils.to_categorical(validate.iloc[:,col], num_classes=NUM_CLASSES).astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "y_test[0][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition (CNN with Gaussian Noise and 5 Output Splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random weights matrix\n",
    "\n",
    "random_weights = np.random.normal(size=(WORDS_SIZE, 13),scale=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must use non-sequential model building to create branches in the output layer\n",
    "inp_layer = tf.keras.layers.Input(shape=(INPUT_SIZE,))\n",
    "mid_layers = tf.keras.layers.Embedding(input_dim = WORDS_SIZE,\n",
    "                                    output_dim = 13,\n",
    "                                    weights=[random_weights],\n",
    "                                    input_length = INPUT_SIZE)(inp_layer)\n",
    "mid_layers = tf.keras.layers.Convolution1D(filters=512, kernel_size=(9), padding='same', activation='relu')(mid_layers)\n",
    "mid_layers = tf.keras.layers.MaxPool1D(pool_size=5)(mid_layers)\n",
    "mid_layers = tf.keras.layers.Dropout(0.5)(mid_layers)\n",
    "mid_layers = tf.keras.layers.Flatten()(mid_layers)\n",
    "mid_layers = tf.keras.layers.Dense(64, activation='relu')(mid_layers)\n",
    "mid_layers = tf.keras.layers.Dense(16, activation='relu')(mid_layers)\n",
    "output1 = tf.keras.layers.Dense(2, activation='softmax')(mid_layers)\n",
    "output2 = tf.keras.layers.Dense(2, activation='softmax')(mid_layers)\n",
    "output3 = tf.keras.layers.Dense(2, activation='softmax')(mid_layers)\n",
    "output4 =tf.keras.layers.Dense(2, activation='softmax')(mid_layers)\n",
    "output5 = tf.keras.layers.Dense(2, activation='softmax')(mid_layers)\n",
    "model = tf.keras.Model(inp_layer,[output1,output2,output3,output4,output5])\n",
    "\n",
    "# Define custom optimizers\n",
    "adam = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1, decay=0.0, amsgrad=False)\n",
    "\n",
    "## Compile model with metrics\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"CNN model built: \")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create TensorBoard callbacks\n",
    "\n",
    "callbackdir= 'D:\\\\temp\\\\cb'\n",
    "\n",
    "tbCallback = tf.keras.callbacks.TensorBoard(log_dir=callbackdir, \n",
    "                         histogram_freq=1,\n",
    "                         embeddings_freq=1,\n",
    "                         write_graph=True, \n",
    "                         write_images=True)\n",
    "\n",
    "tbCallback.set_model(model)\n",
    "mld = 'model/model-ALL-{epoch:02d}.hdf5'\n",
    "\n",
    "## Create best model callback\n",
    "mcp = tf.keras.callbacks.ModelCheckpoint(filepath=mld, \n",
    "                                         monitor=\"val_loss\",\n",
    "                                         save_best_only=True, \n",
    "                                         mode='auto', \n",
    "                                         save_freq='epoch', \n",
    "                                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "class_weights = [{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.},{0: 1., 1: 5.}]\n",
    "\n",
    "# history = model.fit(x = x_train,\n",
    "#           y = [y_train[0], y_train[1], y_train[2], y_train[3], y_train[4]],\n",
    "#           validation_data = (x_validate, [y_validate[0], y_validate[1], y_validate[2], y_validate[3], y_validate[4]]),\n",
    "#           epochs = 40,\n",
    "#           batch_size = 128,\n",
    "#           verbose =2,\n",
    "#           class_weight= class_weights,\n",
    "#           callbacks=[mcp,tbCallback])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_validate, y_validate),\n",
    "                    callbacks=[tbCallback, mcp])\n",
    "\n",
    "with open('history/History-ALL-40EP-CNN', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation using Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = tf.keras.models.load_model(\"model/model-ALL-01.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "for num in range(0,len(model.metrics_names)):\n",
    "    print(model.metrics_names[num]+': '+str(results[num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check The Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = [[],[],[],[],[]]\n",
    "\n",
    "for col in range(0,len(predicted)):\n",
    "    for row in predicted[col]:\n",
    "        if row[0] >= row[1]:\n",
    "            pred_test[col].append(0)\n",
    "        else:\n",
    "            pred_test[col].append(1)\n",
    "            \n",
    "for col in range(0,len(predicted)):\n",
    "    print(pd.value_counts(pred_test[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in range(1,6):\n",
    "    print('\\nThis is evaluation for column',col)\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])\n",
    "    print(confusion)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    print('\\nTP:',tp)\n",
    "    print('FP:',fp)\n",
    "    print('TN:',tn)\n",
    "    print('FN:',fn)\n",
    "\n",
    "    ## Performance measure\n",
    "    print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])))\n",
    "    print('Precision: '+ str(sklearn.metrics.precision_score(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])))\n",
    "    print('Recall: '+ str(sklearn.metrics.recall_score(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])))\n",
    "    print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])))\n",
    "    print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=test.iloc[:,col].to_numpy(), y_score=predicted[col-1][:,1])))\n",
    "    print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=test.iloc[:,col].to_numpy(), y_score=predicted[col-1][:,1])))\n",
    "    print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=test.iloc[:,col].to_numpy(), y_pred=pred_test[col-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The Model's Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(len(history.history[model.metrics_names[1]]))\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20,15))\n",
    "fig.suptitle('CNN with 10 Epochs')\n",
    "\n",
    "axs[0,0].plot(epochs_range, history.history['val_%s'%(model.metrics_names[6])], 'b', label='CWE-119', color='green')\n",
    "axs[0,0].plot(epochs_range, history.history['val_%s'%(model.metrics_names[7])], 'b', label='CWE-120', color='blue')\n",
    "axs[0,0].plot(epochs_range, history.history['val_%s'%(model.metrics_names[8])], 'b', label='CWE-469', color='red')\n",
    "axs[0,0].plot(epochs_range, history.history['val_%s'%(model.metrics_names[9])], 'b', label='CWE-479', color='purple')\n",
    "axs[0,0].plot(epochs_range, history.history['val_%s'%(model.metrics_names[10])], 'b', label='CWE-Other', color='orange')\n",
    "axs[0,0].set_title('Training accuracy')\n",
    "axs[0,0].legend()\n",
    "\n",
    "\n",
    "axs[0,1].plot(epochs_range, history.history['val_%s'%(model.metrics_names[1])], 'b', label='CWE-119', color='green')\n",
    "axs[0,1].plot(epochs_range, history.history['val_%s'%(model.metrics_names[2])], 'b', label='CWE-120', color='blue')\n",
    "axs[0,1].plot(epochs_range, history.history['val_%s'%(model.metrics_names[3])], 'b', label='CWE-469', color='red')\n",
    "axs[0,1].plot(epochs_range, history.history['val_%s'%(model.metrics_names[4])], 'b', label='CWE-479', color='purple')\n",
    "axs[0,1].plot(epochs_range, history.history['val_%s'%(model.metrics_names[5])], 'b', label='CWE-Other', color='orange')\n",
    "axs[0,1].set_title('Training Loss')\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1,0].plot(epochs_range, history.history[model.metrics_names[6]], 'b', label='CWE-119', color='green')\n",
    "axs[1,0].plot(epochs_range, history.history[model.metrics_names[7]], 'b', label='CWE-120', color='blue')\n",
    "axs[1,0].plot(epochs_range, history.history[model.metrics_names[8]], 'b', label='CWE-469', color='red')\n",
    "axs[1,0].plot(epochs_range, history.history[model.metrics_names[9]], 'b', label='CWE-479', color='purple')\n",
    "axs[1,0].plot(epochs_range, history.history[model.metrics_names[10]], 'b', label='CWE-Other', color='orange')\n",
    "axs[1,0].set_title('Validation accuracy')\n",
    "axs[1,0].legend()\n",
    "\n",
    "\n",
    "axs[1,1].plot(epochs_range, history.history[model.metrics_names[1]], 'b', label='CWE-119', color='green')\n",
    "axs[1,1].plot(epochs_range, history.history[model.metrics_names[2]], 'b', label='CWE-120', color='blue')\n",
    "axs[1,1].plot(epochs_range, history.history[model.metrics_names[3]], 'b', label='CWE-469', color='red')\n",
    "axs[1,1].plot(epochs_range, history.history[model.metrics_names[4]], 'b', label='CWE-479', color='purple')\n",
    "axs[1,1].plot(epochs_range, history.history[model.metrics_names[5]], 'b', label='CWE-Other', color='orange')\n",
    "axs[1,1].set_title('Validation Loss')\n",
    "axs[1,1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('FYP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4f22379b8d6ca0aaa5e64d2ebfc1657349199a8adce4ccf73ce577bcea6099a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
